> ## Environment
1. 先完成了GameEnv的游戏逻辑，可视化，以及游戏的初始化，参数传递等，期间完成了EnvUtils
2. 假设Action和State完成的情况下，完成step函数
3. 完成Action，因为Action简单
4. 完成State
5. 彻底完成GameEnv
6. 写一个简单的操作接口，测试GameEnv的正确性
   1. 初始化的测试
   2. 状态转移的测试（环境可以正确地响应动作，障碍物可以按照期望的方式移动）
   3. 事件的测试（环境可以正确触发吃到食物，碰壁以及碰到障碍物）
   4. 重置的测试（环境可以正确重置）
   5. 奖励的测试（环境可以正确给出奖励）
   6. State测试（State是期望的状态）
> ## Agent
1. 完成ModelUtils
2. 完成神经网络模型A2CModel
3. 测试神经网络能否跑通
4. 完成AgentUtils
5. 开始写A2CAgent，在过程中逐步看需要哪些参数，加入到A2CConfig中
6. 完成Update函数，这里需要MemoryTuple，在AgentUtils里完成MemoryTuple
7. 完成getAction函数，发现Agent需要一个randomAction函数
8. 完成getActionProb函数
> ## Train
1. 完成TrainUtils，完成Logger，完成Memory，完成MovingAverage
2. 完成玩episode的函数，发觉Agent最好有个train()和eval()函数
3. 完成执行训练的函数，发现agent最好有个save
4. 测试Train
   1. 测试过程中，发现在初始化State时，agent_pos可以超出边界1格，这是因为在超出边界时，游戏结束，但还是要获取State，解决方案是在State初始化时额外加1的padding
   2. 在训练过程中，发现智能体总是无脑往上走然后跑出图外
   3. 发现还是得有可视化，于是加入了可视化
5. 完成Evaluate